# Main Hydra configuration
defaults:
  - _self_

# Project paths
paths:
  data:
    raw: "${hydra:runtime.cwd}/data/raw"
    processed: "${hydra:runtime.cwd}/data/processed"
    train: "${hydra:runtime.cwd}/data/processed/train"
    val: "${hydra:runtime.cwd}/data/processed/val" 
    test: "${hydra:runtime.cwd}/data/processed/test"
  models:
    checkpoints: "${hydra:runtime.cwd}/models/checkpoints"
    exported: "${hydra:runtime.cwd}/models/exported"
  logs:
    tensorboard: "${hydra:runtime.cwd}/logs/tensorboard"
  plots: "${hydra:runtime.cwd}/plots"

# Feature processing
feature_processing:
  one_hot_encode: true

# Data configuration
data:
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15

# Dataloader configuration
dataloader:
  batch_size: 128
  num_workers: 4
  pin_memory: true

# Model configuration
architecture: "mlp"
num_classes: 2  # edible or poisonous
input_size: 117  # Will be updated dynamically based on one-hot encoding
hidden_sizes: [256, 128, 64]
dropout_rate: 0.3

# Optimizer configuration
optimizer:
  name: "adam"
  lr: 0.001
  weight_decay: 0.0001

# Scheduler configuration
scheduler:
  name: "reduce_on_plateau"
  factor: 0.5
  patience: 5
  min_lr: 0.00001

# Early stopping configuration
early_stopping:
  monitor: "val_loss"
  patience: 10
  mode: "min"

# Checkpoint configuration
checkpoint:
  dirpath: "${paths.models.checkpoints}"
  filename: "mushroom_classifier-{epoch:02d}-{val_loss:.2f}"
  monitor: "val_loss"
  mode: "min"
  save_top_k: 3
  save_last: true

# Trainer configuration
trainer:
  max_epochs: 100
  accelerator: "auto"
  devices: 1
  precision: 32
  deterministic: true
  gradient_clip_val: 0.5

# Inference configuration
inference:
  device: "cpu"
  input: null
  output: "predictions.json"
  use_onnx: false

# General settings
seed: 42
debug: false

# Experiment tracking
experiment:
  name: "mushroom-classifier"
  tags:
    project: "mushroom-classification"
    task: "tabular-classification"

# Override using command line: python -m mushroom_classifier.train dataloader.batch_size=64 trainer.max_epochs=50
hydra:
  run:
    dir: ${hydra:runtime.cwd}/outputs/${experiment.name}/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${hydra:runtime.cwd}/outputs/${experiment.name}_sweep/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num} 